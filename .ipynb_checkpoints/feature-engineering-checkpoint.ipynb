{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffca79dc-85b7-406e-ba2d-a68b4fa0aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libararies\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c30944-0a52-4f23-9141-d72bb51dad0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "0        0001_01     Europa     False     B/0/P    TRAPPIST-1e  39.0  False   \n",
      "1        0002_01      Earth     False     F/0/S    TRAPPIST-1e  24.0  False   \n",
      "2        0003_01     Europa     False     A/0/S    TRAPPIST-1e  58.0   True   \n",
      "3        0003_02     Europa     False     A/0/S    TRAPPIST-1e  33.0  False   \n",
      "4        0004_01      Earth     False     F/1/S    TRAPPIST-1e  16.0  False   \n",
      "...          ...        ...       ...       ...            ...   ...    ...   \n",
      "8688     9276_01     Europa     False    A/98/P    55 Cancri e  41.0   True   \n",
      "8689     9278_01      Earth      True  G/1499/S  PSO J318.5-22  18.0  False   \n",
      "8690     9279_01      Earth     False  G/1500/S    TRAPPIST-1e  26.0  False   \n",
      "8691     9280_01     Europa     False   E/608/S    55 Cancri e  32.0  False   \n",
      "8692     9280_02     Europa     False   E/608/S    TRAPPIST-1e  44.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0             0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1           109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2            43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3             0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4           303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "...           ...        ...           ...     ...     ...                ...   \n",
      "8688          0.0     6819.0           0.0  1643.0    74.0  Gravior Noxnuther   \n",
      "8689          0.0        0.0           0.0     0.0     0.0    Kurta Mondalley   \n",
      "8690          0.0        0.0        1872.0     1.0     0.0       Fayey Connon   \n",
      "8691          0.0     1049.0           0.0   353.0  3235.0   Celeon Hontichre   \n",
      "8692        126.0     4688.0           0.0     0.0    12.0   Propsh Hontichre   \n",
      "\n",
      "      Transported  \n",
      "0           False  \n",
      "1            True  \n",
      "2           False  \n",
      "3           False  \n",
      "4            True  \n",
      "...           ...  \n",
      "8688        False  \n",
      "8689        False  \n",
      "8690         True  \n",
      "8691        False  \n",
      "8692         True  \n",
      "\n",
      "[8693 rows x 14 columns]\n",
      "     PassengerId HomePlanet CryoSleep     Cabin    Destination   Age    VIP  \\\n",
      "0        0013_01      Earth      True     G/3/S    TRAPPIST-1e  27.0  False   \n",
      "1        0018_01      Earth     False     F/4/S    TRAPPIST-1e  19.0  False   \n",
      "2        0019_01     Europa      True     C/0/S    55 Cancri e  31.0  False   \n",
      "3        0021_01     Europa     False     C/1/S    TRAPPIST-1e  38.0  False   \n",
      "4        0023_01      Earth     False     F/5/S    TRAPPIST-1e  20.0  False   \n",
      "...          ...        ...       ...       ...            ...   ...    ...   \n",
      "4272     9266_02      Earth      True  G/1496/S    TRAPPIST-1e  34.0  False   \n",
      "4273     9269_01      Earth     False       NaN    TRAPPIST-1e  42.0  False   \n",
      "4274     9271_01       Mars      True   D/296/P    55 Cancri e   NaN  False   \n",
      "4275     9273_01     Europa     False   D/297/P            NaN   NaN  False   \n",
      "4276     9277_01      Earth      True  G/1498/S  PSO J318.5-22  43.0  False   \n",
      "\n",
      "      RoomService  FoodCourt  ShoppingMall     Spa  VRDeck              Name  \n",
      "0             0.0        0.0           0.0     0.0     0.0   Nelly Carsoning  \n",
      "1             0.0        9.0           0.0  2823.0     0.0    Lerome Peckers  \n",
      "2             0.0        0.0           0.0     0.0     0.0   Sabih Unhearfus  \n",
      "3             0.0     6652.0           0.0   181.0   585.0  Meratz Caltilter  \n",
      "4            10.0        0.0         635.0     0.0     0.0   Brence Harperez  \n",
      "...           ...        ...           ...     ...     ...               ...  \n",
      "4272          0.0        0.0           0.0     0.0     0.0       Jeron Peter  \n",
      "4273          0.0      847.0          17.0    10.0   144.0     Matty Scheron  \n",
      "4274          0.0        0.0           0.0     0.0     0.0       Jayrin Pore  \n",
      "4275          0.0     2680.0           0.0     0.0   523.0    Kitakan Conale  \n",
      "4276          0.0        0.0           0.0     0.0     0.0  Lilace Leonzaley  \n",
      "\n",
      "[4277 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "work_dir = Path.cwd()\n",
    "train_df = pd.read_csv(work_dir/\"kaggle-data\"/\"train.csv\")\n",
    "print(train_df)\n",
    "test_df = pd.read_csv(work_dir/\"kaggle-data\"/\"test.csv\")\n",
    "print(test_df)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e86f8537-9b56-4484-a010-8d40558aa2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cabin_num_to_bins(num):\n",
    "    try:\n",
    "        num = int(num)\n",
    "    except ValueError:\n",
    "        return pd.NA\n",
    "    if num <= 300:\n",
    "        return 1\n",
    "    elif num <= 600:\n",
    "        return 2\n",
    "    elif num <= 900:\n",
    "        return 3\n",
    "    elif num <= 1200:\n",
    "        return 4\n",
    "    elif num <= 1500:\n",
    "        return 5\n",
    "    else:\n",
    "        return 6\n",
    "\n",
    "def process_data(df, file_name, scaler_name, save_scaler=False):\n",
    "    # splitting passenger id and cabin information into multiple columns, removing not needed columns\n",
    "    df[\"PassengerGroup\"] = df[\"PassengerId\"].str.split(\"_\").str[0]\n",
    "    df[\"GroupSize\"] = df.groupby(\"PassengerGroup\")[\"PassengerGroup\"].transform(\"count\")\n",
    "\n",
    "    df[\"CabinDeck\"] = df[\"Cabin\"].str.split(\"/\").str[0]\n",
    "    df[\"CabinNum\"] = df[\"Cabin\"].str.split(\"/\").str[1]\n",
    "    df[\"CabinSide\"] = df[\"Cabin\"].str.split(\"/\").str[2]\n",
    "\n",
    "    df.drop([\"PassengerId\", \"Cabin\", \"Name\", \"PassengerGroup\"], axis=1, inplace=True)\n",
    "\n",
    "    # gathering CabinNum into bins\n",
    "    df[\"CabinNumBin\"] = df[\"CabinNum\"].map(map_cabin_num_to_bins)\n",
    "    df.drop(\"CabinNum\", axis=1, inplace=True)\n",
    "\n",
    "    # one hot encoding \"HomePlanet\", \"Destination\", \"CabinDeck\", \"CabinSide\" and \"CabinNumBin\"\n",
    "    columns_to_encode = [\"HomePlanet\", \"Destination\", \"CabinDeck\", \"CabinSide\", \"CabinNumBin\"]\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_array = encoder.fit_transform(df[columns_to_encode])\n",
    "    encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "    df.drop(columns_to_encode, axis=1, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "    # changing columns CryoSleep and VIP to bool type\n",
    "    train_df[[\"CryoSleep\", \"VIP\"]] = train_df[[\"CryoSleep\", \"VIP\"]].astype(bool)\n",
    "\n",
    "    # scaling columns containing numerical values\n",
    "    columns_to_scale = [\"Age\", \"RoomService\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\", \"GroupSize\"]\n",
    "    if save_scaler:\n",
    "        scaler = MinMaxScaler()\n",
    "        df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "        joblib.dump(scaler, scaler_name + \".joblib\")\n",
    "    else:\n",
    "        scaler = joblib.load(scaler_name + \".joblib\")\n",
    "        df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "    df.to_csv(file_name + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a59db80-f127-412e-919f-6171650958a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoders require their input argument must be uniformly strings or numbers. Got ['NAType', 'int']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/utils/_encode.py:183\u001b[0m, in \u001b[0;36m_unique_python\u001b[0;34m(values, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    181\u001b[0m uniques_set, missing_values \u001b[38;5;241m=\u001b[39m _extract_missing(uniques_set)\n\u001b[0;32m--> 183\u001b[0m uniques \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muniques_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m uniques\u001b[38;5;241m.\u001b[39mextend(missing_values\u001b[38;5;241m.\u001b[39mto_list())\n",
      "File \u001b[0;32mmissing.pyx:392\u001b[0m, in \u001b[0;36mpandas._libs.missing.NAType.__bool__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: boolean value of NA is ambiguous",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# test data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# dataset_01_test = test_df.dropna().copy() # rows shouldnt be deleted from the test dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dataset_01_test \u001b[38;5;241m=\u001b[39m test_df\n\u001b[0;32m----> 8\u001b[0m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_01_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataset_01_test\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscaler_01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(df, file_name, scaler_name, save_scaler)\u001b[0m\n\u001b[1;32m     35\u001b[0m columns_to_encode \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHomePlanet\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDestination\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCabinDeck\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCabinSide\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCabinNumBin\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     36\u001b[0m encoder \u001b[38;5;241m=\u001b[39m OneHotEncoder(sparse_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 37\u001b[0m encoded_array \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns_to_encode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m encoded_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(encoded_array, columns\u001b[38;5;241m=\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget_feature_names_out(columns_to_encode))\n\u001b[1;32m     39\u001b[0m df\u001b[38;5;241m.\u001b[39mdrop(columns_to_encode, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/utils/_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    325\u001b[0m         )\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    904\u001b[0m             (\n\u001b[1;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    914\u001b[0m         )\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:991\u001b[0m, in \u001b[0;36mOneHotEncoder.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    974\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m    Fit OneHotEncoder to X.\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;124;03m        Fitted encoder.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle_unknown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_unknown\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_drop_idx()\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features_outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_n_features_outs()\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/preprocessing/_encoders.py:103\u001b[0m, in \u001b[0;36m_BaseEncoder._fit\u001b[0;34m(self, X, handle_unknown, ensure_all_finite, return_counts, return_and_ignore_missing_for_infrequent)\u001b[0m\n\u001b[1;32m    100\u001b[0m Xi \u001b[38;5;241m=\u001b[39m X_list[i]\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategories \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_unique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_counts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compute_counts:\n\u001b[1;32m    105\u001b[0m         cats, counts \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/utils/_encode.py:52\u001b[0m, in \u001b[0;36m_unique\u001b[0;34m(values, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper function to find unique values with support for python objects.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03mUses pure python method for object dtype, and numpy method for\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    array. Only provided if `return_counts` is True.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_unique_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# numerical\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _unique_np(\n\u001b[1;32m     57\u001b[0m     values, return_inverse\u001b[38;5;241m=\u001b[39mreturn_inverse, return_counts\u001b[38;5;241m=\u001b[39mreturn_counts\n\u001b[1;32m     58\u001b[0m )\n",
      "File \u001b[0;32m/usr/lib/python3.13/site-packages/sklearn/utils/_encode.py:188\u001b[0m, in \u001b[0;36m_unique_python\u001b[0;34m(values, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(t\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mtype\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values))\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoders require their input argument must be uniformly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings or numbers. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m ret \u001b[38;5;241m=\u001b[39m (uniques,)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_inverse:\n",
      "\u001b[0;31mTypeError\u001b[0m: Encoders require their input argument must be uniformly strings or numbers. Got ['NAType', 'int']"
     ]
    }
   ],
   "source": [
    "# first dataset - all rows containing NA values removed\n",
    "# train data\n",
    "dataset_01_train = train_df.dropna().copy()\n",
    "process_data(df=dataset_01_train, file_name=\"dataset_01_train\", scaler_name=\"scaler_01\", save_scaler=True)\n",
    "# test data\n",
    "# dataset_01_test = test_df.dropna().copy() # rows shouldnt be deleted from the test dataset\n",
    "dataset_01_test = test_df\n",
    "# missing categorical data with be imputed in a way that conserves proportions\n",
    "dataset_01_test[\"CryoSleep\"] = dataset_01_test[\"CryoSleep\"].apply(lambda x: random.choice(dataset_01_test[\"CryoSleep\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_01_test[\"HomePlanet\"] = dataset_01_test[\"HomePlanet\"].apply(lambda x: random.choice(dataset_01_test[\"HomePlanet\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_01_test[\"Cabin\"] = dataset_01_test[\"Cabin\"].apply(lambda x: random.choice(dataset_01_test[\"Cabin\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_01_test[\"Destination\"] = dataset_01_test[\"Destination\"].apply(lambda x: random.choice(dataset_01_test[\"Destination\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_01_test[\"VIP\"] = dataset_01_test[\"VIP\"].apply(lambda x: random.choice(dataset_01_test[\"VIP\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "\n",
    "# missing values from the numerical columns imputed with the median\n",
    "dataset_01_test[\"Age\"] = dataset_01_test[\"Age\"].fillna(dataset_01[\"Age\"].median())\n",
    "dataset_01_test[\"RoomService\"] = dataset_01_test[\"RoomService\"].fillna(dataset_01_test[\"RoomService\"].median())\n",
    "dataset_01_test[\"FoodCourt\"] = dataset_01_test[\"FoodCourt\"].fillna(dataset_01_test[\"FoodCourt\"].median())\n",
    "dataset_01_test[\"ShoppingMall\"] = dataset_01_test[\"ShoppingMall\"].fillna(dataset_01_test[\"ShoppingMall\"].median())\n",
    "dataset_01_test[\"Spa\"] = dataset_01_test[\"Spa\"].fillna(dataset_01_test[\"Spa\"].median())\n",
    "dataset_01_test[\"VRDeck\"] = dataset_01_test[\"VRDeck\"].fillna(dataset_01_test[\"VRDeck\"].median())\n",
    "process_data(df=dataset_01_test, file_name=\"dataset_01_test\", scaler_name=\"scaler_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71a56c-f730-4b8a-9b0e-2c528da62f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second dataset - removing NA values from the CryoSleep column, imputing the rest\n",
    "#train data\n",
    "dataset_02_train = train_df.dropna(subset=\"CryoSleep\").copy()\n",
    "# missing categorical data with be imputed in a way that conserves proportions\n",
    "dataset_02_train[\"HomePlanet\"] = dataset_02_train[\"HomePlanet\"].apply(lambda x: random.choice(dataset_02_train[\"HomePlanet\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_train[\"Cabin\"] = dataset_02_train[\"Cabin\"].apply(lambda x: random.choice(dataset_02_train[\"Cabin\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_train[\"Destination\"] = dataset_02_train[\"Destination\"].apply(lambda x: random.choice(dataset_02_train[\"Destination\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_train[\"VIP\"] = dataset_02_train[\"VIP\"].apply(lambda x: random.choice(dataset_02_train[\"VIP\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "\n",
    "# missing values from the numerical columns imputed with the median\n",
    "dataset_02_train[\"Age\"] = dataset_02_train[\"Age\"].fillna(dataset_02[\"Age\"].median())\n",
    "dataset_02_train[\"RoomService\"] = dataset_02_train[\"RoomService\"].fillna(dataset_02_train[\"RoomService\"].median())\n",
    "dataset_02_train[\"FoodCourt\"] = dataset_02_train[\"FoodCourt\"].fillna(dataset_02_train[\"FoodCourt\"].median())\n",
    "dataset_02_train[\"ShoppingMall\"] = dataset_02_train[\"ShoppingMall\"].fillna(dataset_02_train[\"ShoppingMall\"].median())\n",
    "dataset_02_train[\"Spa\"] = dataset_02_train[\"Spa\"].fillna(dataset_02_train[\"Spa\"].median())\n",
    "dataset_02_train[\"VRDeck\"] = dataset_02_train[\"VRDeck\"].fillna(dataset_02_train[\"VRDeck\"].median())\n",
    "process_data(df=dataset_02_train, file_name=\"dataset_02_train\", save_scaler=True, scaler_name=\"scaler_02\")\n",
    "\n",
    "#test data\n",
    "dataset_02_test = test_df\n",
    "# missing categorical data with be imputed in a way that conserves proportions\n",
    "dataset_02_test[\"CryoSleep\"] = dataset_02_test[\"CryoSleep\"].apply(lambda x: random.choice(dataset_02_test[\"CryoSleep\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_test[\"HomePlanet\"] = dataset_02_test[\"HomePlanet\"].apply(lambda x: random.choice(dataset_02_test[\"HomePlanet\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_test[\"Cabin\"] = dataset_02_test[\"Cabin\"].apply(lambda x: random.choice(dataset_02_test[\"Cabin\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_test[\"Destination\"] = dataset_02_test[\"Destination\"].apply(lambda x: random.choice(dataset_02_test[\"Destination\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "dataset_02_test[\"VIP\"] = dataset_02_test[\"VIP\"].apply(lambda x: random.choice(dataset_02_test[\"VIP\"].dropna().tolist()) if pd.isna(x) else x)\n",
    "\n",
    "# missing values from the numerical columns imputed with the median\n",
    "dataset_02_test[\"Age\"] = dataset_02_test[\"Age\"].fillna(dataset_02[\"Age\"].median())\n",
    "dataset_02_test[\"RoomService\"] = dataset_02_test[\"RoomService\"].fillna(dataset_02_test[\"RoomService\"].median())\n",
    "dataset_02_test[\"FoodCourt\"] = dataset_02_test[\"FoodCourt\"].fillna(dataset_02_test[\"FoodCourt\"].median())\n",
    "dataset_02_test[\"ShoppingMall\"] = dataset_02_test[\"ShoppingMall\"].fillna(dataset_02_test[\"ShoppingMall\"].median())\n",
    "dataset_02_test[\"Spa\"] = dataset_02_test[\"Spa\"].fillna(dataset_02_test[\"Spa\"].median())\n",
    "dataset_02_test[\"VRDeck\"] = dataset_02_test[\"VRDeck\"].fillna(dataset_02_test[\"VRDeck\"].median())\n",
    "process_data(df=dataset_02_test, file_name=\"dataset_02_test\", scaler_name=\"scaler_02\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
